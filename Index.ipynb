{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "sys.path.append(\"./semi-supervised-pytorch/semi-supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\VAE-for-WiFi-Indoor-Localization\\./semi-supervised-pytorch/semi-supervised\\models\\vae.py:114: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (sample): GaussianSample(\n",
       "      (mu): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (log_var): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (reconstruction): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (output_activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import VariationalAutoencoder\n",
    "from layers import GaussianSample\n",
    "model = VariationalAutoencoder([512, 128, [256,]])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample -0.60 drawn from N(-0.69, 1.61)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "gaussian = GaussianSample(10, 1)\n",
    "z, mu, log_var = gaussian(Variable(torch.ones(1, 10)))\n",
    "\n",
    "print(f\"sample {float(z.data):.2f} drawn from N({float(mu.data):.2f}, {float(log_var.exp().data):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8089]], grad_fn=<AddcmulBackward0>),\n",
       " tensor([[-0.6889]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[0.4750]], grad_fn=<SoftplusBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian(Variable(torch.ones(1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Computes the KL-divergence of\n",
      "        some element z.\n",
      "\n",
      "        KL(q||p) = -âˆ« q(z) log [ p(z) / q(z) ]\n",
      "                 = -E[log p(z) - log q(z)]\n",
      "\n",
      "        :param z: sample from q-distribuion\n",
      "        :param q_param: (mu, log_var) of the q-distribution\n",
      "        :param p_param: (mu, log_var) of the p-distribution\n",
      "        :return: KL(q||p)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(model._kld.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UJIDataset import UJIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this custom BCE function until PyTorch implements reduce=False\n",
    "def binary_cross_entropy(r, x):\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "tr_dataset = UJIDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=tr_dataset, batch_size=128, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (u, _) in enumerate(train_loader):\n",
    "        u = Variable(u)\n",
    "\n",
    "        reconstruction = model(u)\n",
    "        \n",
    "        likelihood = -binary_cross_entropy(reconstruction, u)\n",
    "        elbo = likelihood - model.kl_divergence\n",
    "        \n",
    "        L = -torch.mean(elbo)\n",
    "\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += L.data[0]\n",
    "\n",
    "    m = len(train_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch}\\tL: {total_loss/m:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3a5103e3b492ba7af4f71d1553195c04e9f5c9c31b45e8009e6c12cc9874818"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
